---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from datetime import datetime

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# Required libraries for web scraping and data handling
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Define the target URL for scraping data
oig_url = 'https://oig.hhs.gov/fraud/enforcement/'

# Initiate a GET request to fetch data from the URL
oig_web_data = requests.get(oig_url)

# Utilize BeautifulSoup to parse the fetched HTML content
oig_parsed_html = BeautifulSoup(oig_web_data.text, 'html.parser')

# Prepare an empty list to hold the data of enforcement actions
collected_enforcement_data = []

# Iterate over each element in the section containing the data
for element in oig_parsed_html.find_all('li', class_='usa-card'):
    # Locate and clean the title and link, ensuring all entries are captured even if some details are missing
    title_element = element.find('a')
    title = title_element.text.strip() if title_element else 'No title found'
    link = title_element['href'] if title_element else 'No link found'

    # Search for and format the date, managing missing entries effectively
    date_element = element.find('span', class_='text-base-dark')
    date = date_element.text.strip() if date_element else 'No date found'

    # Identify and strip the category, accounting for possible absences
    category_element = element.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_element.text.strip() if category_element else 'No category found'

    # Append the collected details to the list
    collected_enforcement_data.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link
    })

# Transform the collected list of dictionaries into a DataFrame for better data management
enforcement_dataframe = pd.DataFrame(collected_enforcement_data)

# Print the top rows of the DataFrame to verify correct data collection
print(enforcement_dataframe.head())

# Export the DataFrame to a CSV file in the current directory
enforcement_dataframe.to_csv('enforcement_actions.csv', index=False)
```

  
### 2. Crawling (PARTNER 1)

```{python}
# List to hold data with additional 'Agency' field
collected_data_with_agency = []

# Iterate over each enforcement action entry found in parsed HTML
for element in oig_parsed_html.find_all('li', class_='usa-card'):
    title_element = element.find('a')
    title = title_element.text.strip() if title_element else 'No title found'
    link = 'https://oig.hhs.gov' + title_element['href'] if title_element else 'No link found'
    
    # Fetch and parse the linked page for agency information
    if title_element:
        agency_response = requests.get(link)
        agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
        agency_info = agency_soup.find(lambda tag: tag.name == "span" and 'Agency:' in tag.text)
        agency_name = agency_info.next_sibling.text.strip() if agency_info and agency_info.next_sibling else 'No agency found'
    else:
        agency_name = 'No agency found'

    # Use previously parsed date and category information
    date_element = element.find('span', class_='text-base-dark')
    date = date_element.text.strip() if date_element else 'No date found'

    category_element = element.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_element.text.strip() if category_element else 'No category found'

    # Append the additional details to the list
    collected_data_with_agency.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link,
        'Agency': agency_name
    })

# Create a DataFrame and save to CSV
detailed_dataframe = pd.DataFrame(collected_data_with_agency)
print(detailed_dataframe.head())
detailed_dataframe.to_csv('enforcement_actions_detailed.csv', index=False)
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

Define the function and its 2 inputs 

Check for valid year

Use an if statement: if start_year earlier than 2013, print a message telling user its invalid. 
No else statement needed.

Setup base_url and an empty list as in previous code.

Create datetime object from inputs and grab today's date as a datetime object.

Use a While loop. 
The loop should go through the site's pages until it finds a date outside of the range specified. That is, it should start with the current date and move backwards until it finds a date older than the function inputs. But we can't do that here. It makes more sense to do it in the For loop below.
The While loop also needs a counter to move forward through the website's pages.

Use requests.get() to fetch the data from the page.

Then, using the code above as a guide...

Use BeautifulSoup to parse the HTML content of the page. This code to be based on the code above.

Begin a For loop. For each action find:
  Title and Link
  Date
  Category
  Agency Information:
This is where we create the conditions to exit the While loop. If the date is older than the inputs of the function, the function should just return what its got and stop looping.

Create a dictionary of the actions.
Append this dictionary to empty list.

Use time.sleep(1) to wait for 1 second.

Increment the page counter by 1 to go to next page.

After the loop, convert the created list (of dictionaries) to a DataFrame.
Save the DataFrame as a csv.

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
def scrape_function(start_month, start_year):
    # Check that the year is within bounds
    if start_year < 2013:
        print("Please restrict the year to 2013 or later.")
        return
    
    # Define the base URL
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    collected_data = []
    
    # Convert start date to datetime and determine today's date
    start_date = datetime(start_year, start_month, 1)
    today_date = datetime.today()
    
    # Paginate through results until we reach the start_date
    page = 1
    while True:
        # Fetch data for the current page
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        if response.status_code != 200:
            print("Failed to retrieve data.")
            break
        
        # Parse the HTML content
        soup = BeautifulSoup(response.text, 'html.parser')
        actions = soup.find_all('li', class_='usa-card')
        
        # Process each enforcement action item
        for action in actions:
            title_element = action.find('a')
            title = title_element.text.strip() if title_element else 'No title found'
            link = 'https://oig.hhs.gov' + title_element['href'] if title_element else 'No link found'

            # Extract and parse the date
            date_element = action.find('span', class_='text-base-dark')
            date_str = date_element.text.strip() if date_element else 'No date found'
            date_obj = datetime.strptime(date_str, '%B %d, %Y') if date_element else None
            
            # Break out of loop if the date is before start_date
            if date_obj and date_obj < start_date:
                # Convert to DataFrame and return if we've reached the start date
                df = pd.DataFrame(collected_data)
                filename = f"enforcement_actions_{start_year}_{start_month:02d}.csv"
                df.to_csv(filename, index=False)
                print(f"Data saved to {filename}")
                return df
            
            # Extract category
            category_element = action.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
            category = category_element.text.strip() if category_element else 'No category found'
            
            # Visit link to extract agency info
            agency_name = 'No agency found'
            if link != 'No link found':
                agency_response = requests.get(link)
                agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
                agency_info = agency_soup.find(lambda tag: tag.name == "span" and 'Agency:' in tag.text)
                if agency_info and agency_info.next_sibling and hasattr(agency_info.next_sibling, 'text'):
                    agency_name = agency_info.next_sibling.text.strip()

            # Append data
            collected_data.append({
                'Title': title,
                'Date': date_str,
                'Category': category,
                'Link': link,
                'Agency': agency_name
            })
        
        # Wait before going to the next page to avoid server blocks
        time.sleep(1)
        
        # Move to the next page
        page += 1

    # Ensure collected_data is returned as a DataFrame at the end of scraping
    df = pd.DataFrame(collected_data)
    filename = f"enforcement_actions_{start_year}_{start_month:02d}.csv"
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")
    return df

# Example usage of the function
test_df = scrape_function(1, 2023)

# Print total enforcement actions in the DataFrame
print(f"\nTotal enforcement actions: {len(test_df)}")

# Find and print the earliest enforcement action
if not test_df.empty:
    earliest_action = test_df.sort_values(by='Date').iloc[0]
    print("Earliest Enforcement Action:")
    print(earliest_action)
```

* c. Test Partner's Code (PARTNER 1)

```{python}
# Example usage of the function
partner_test_df = scrape_function(1, 2021)

# Print total enforcement actions in the DataFrame
print(f"\nTotal enforcement actions: {len(partner_test_df)}")

# Find and print the earliest enforcement action
if not partner_test_df.empty:
    earliest_action_partner = partner_test_df.sort_values(by='Date').iloc[0]
    print("Earliest Enforcement Action:")
    print(earliest_action_partner)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
import pandas as pd
import altair as alt

# Load the data
file_path = (r'C:\Users\james\Desktop\Kajie-Griffin-ps5\enforcement_actions_2021_01.csv') 
enforcement_data = pd.read_csv(file_path)

# Convert 'Date' column to datetime format to handle monthly grouping
enforcement_data['Date'] = pd.to_datetime(enforcement_data['Date'], errors='coerce')

# Drop rows where 'Date' conversion failed
enforcement_data = enforcement_data.dropna(subset=['Date'])

# Extract month and year from 'Date' for grouping
enforcement_data['YearMonth'] = enforcement_data['Date'].dt.to_period('M')

# Filter the data to include only the specified categories
filtered_data = enforcement_data[enforcement_data['Category'].isin(['Criminal and Civil Actions', 'State Enforcement Agencies'])]

# Group by YearMonth and Category, counting occurrences in each group
filtered_monthly_counts = filtered_data.groupby(['YearMonth', 'Category']).size().unstack(fill_value=0)

# Reset the index and convert YearMonth to string for Altair compatibility
filtered_monthly_counts = filtered_monthly_counts.reset_index()
filtered_monthly_counts['YearMonth'] = filtered_monthly_counts['YearMonth'].astype(str)

# Melt the dataframe for Altair compatibility
filtered_long_df = filtered_monthly_counts.melt(id_vars='YearMonth', 
                                                var_name='Category', 
                                                value_name='Count')

# Create the Altair line chart
chart = alt.Chart(filtered_long_df).mark_line(point=True).encode(
    x=alt.X('YearMonth:T', title='Month'),
    y=alt.Y('Count', title='Number of Enforcement Actions'),
    color=alt.Color('Category', legend=alt.Legend(title='Category')),
    tooltip=['YearMonth', 'Category', 'Count']
).properties(
    title='Monthly Enforcement Actions: Criminal and Civil Actions vs. State Enforcement Agencies',
    width=600,
    height=400
).interactive()

chart.display()
```

* based on five topics

```{python}
# Load the dataset
data = pd.read_csv('/Users/Kaijie/Desktop/Kajie-Griffin-ps5/enforcement_actions_2021_01.csv')

# Function to categorize titles based on keywords
def categorize_title(title):
    title = title.lower()
    if any(word in title for word in ["health", "medical", "hospital", "clinic", "medicare", "medicaid"]):
        return "Health Care Fraud"
    elif any(word in title for word in ["bank", "financial", "money laundering", "fraudulent", "embezzlement"]):
        return "Financial Fraud"
    elif any(word in title for word in ["drug", "pharmacy", "narcotic", "opioid"]):
        return "Drug Enforcement"
    elif any(word in title for word in ["bribery", "corrupt", "kickback"]):
        return "Bribery/Corruption"
    else:
        return "Other"

# Apply categorization function to the title column
data['Topic'] = data['Title'].apply(categorize_title)

# Convert 'Date' to datetime and format it as 'YYYY-MM'
data['Date'] = pd.to_datetime(data['Date']).dt.strftime('%Y-%m')

# Group the data by Topic and Date and count the occurrences
grouped_data = data.groupby(['Topic', 'Date']).size().reset_index(name='Number of Actions')

# Create a line chart using Altair
line_chart = alt.Chart(grouped_data).mark_line(point=True).encode(
    x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%Y-%m')),
    y=alt.Y('Number of Actions:Q', title='Number of Actions'),
    color='Topic:N',
    tooltip=['Topic', 'Date', 'Number of Actions']
).properties(
    title='Number of Enforcement Actions by Topic Over Time',
    width=600,
    height=400
)

# Display the chart
line_chart.display()
```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import re
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable

file_path = (r'C:\Users\james\Desktop\Kajie-Griffin-ps5\enforcement_actions_2021_01.csv') 
data = pd.read_csv(file_path)

# Define U.S. states list
us_states = [
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", 
    "Connecticut", "Delaware", "Florida", "Georgia", "Hawaii", "Idaho", 
    "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", 
    "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota", 
    "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada", 
    "New Hampshire", "New Jersey", "New Mexico", "New York", 
    "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon", 
    "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", 
    "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington", 
    "West Virginia", "Wisconsin", "Wyoming"
]

# Define a function to find a state name in a text string
def get_state_from_text(text):
    text = text.lower()  # Normalize to lowercase for matching
    for state in us_states:
        if re.search(r"\b" + state.lower() + r"\b", text):
            return state
    return None

# Apply the function to extract state names
data['State'] = data['Agency'].apply(get_state_from_text)

# Count occurrences of each state and merge with the full state list
state_counts = data['State'].value_counts().reset_index()
state_counts.columns = ['State', 'Enforcement Actions']

# Ensure all states are included, even those with no actions
state_data = pd.DataFrame(us_states, columns=['State']).merge(state_counts, on='State', how='left')
state_data['Enforcement Actions'].fillna(0, inplace=True)

# Load the shapefile and filter only for relevant states
shape_data = gpd.read_file(r'C:\Users\james\Desktop\cb_2018_us_state_500k\cb_2018_us_state_500k.shp')
shape_data = shape_data[shape_data['NAME'].isin(us_states)]  # Keep only the required states

# Merge state enforcement data with the shapefile data
map_with_data = shape_data.merge(state_data, left_on='NAME', right_on='State', how='left')
map_with_data['Enforcement Actions'].fillna(0, inplace=True)

# Plotting the map
fig, ax = plt.subplots(figsize=(20, 12))  # Use larger size for better visibility

# Use 'plasma' colormap
map_colors = map_with_data.plot(column='Enforcement Actions', ax=ax, cmap='plasma', legend=False)

# Draw state boundaries
map_with_data.boundary.plot(ax=ax, linewidth=1, edgecolor='black')

# Create color bar and adjust position
divider = make_axes_locatable(ax)
color_axis = divider.append_axes("bottom", size="5%", pad=0.5)
color_bar = plt.colorbar(map_colors.collections[0], cax=color_axis, orientation='horizontal')
color_bar.set_label('Enforcement Actions per State', size=12)
color_bar.ax.tick_params(labelsize=10)

# Adjust view to include Alaska and Hawaii
ax.set_xlim(-180, -65)  # Set x-axis range to include Alaska
ax.set_ylim(15, 75)     # Set y-axis range to include Hawaii

# Turn off axis for a cleaner map display
ax.set_axis_off()

plt.show()
```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```
