---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from datetime import datetime

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# Required libraries for web scraping and data handling
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Define the target URL for scraping data
oig_url = 'https://oig.hhs.gov/fraud/enforcement/'

# Initiate a GET request to fetch data from the URL
oig_web_data = requests.get(oig_url)

# Utilize BeautifulSoup to parse the fetched HTML content
oig_parsed_html = BeautifulSoup(oig_web_data.text, 'html.parser')

# Prepare an empty list to hold the data of enforcement actions
collected_enforcement_data = []

# Iterate over each element in the section containing the data
for element in oig_parsed_html.find_all('li', class_='usa-card'):
    # Locate and clean the title and link, ensuring all entries are captured even if some details are missing
    title_element = element.find('a')
    title = title_element.text.strip() if title_element else 'No title found'
    link = title_element['href'] if title_element else 'No link found'

    # Search for and format the date, managing missing entries effectively
    date_element = element.find('span', class_='text-base-dark')
    date = date_element.text.strip() if date_element else 'No date found'

    # Identify and strip the category, accounting for possible absences
    category_element = element.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_element.text.strip() if category_element else 'No category found'

    # Append the collected details to the list
    collected_enforcement_data.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link
    })

# Transform the collected list of dictionaries into a DataFrame for better data management
enforcement_dataframe = pd.DataFrame(collected_enforcement_data)

# Print the top rows of the DataFrame to verify correct data collection
print(enforcement_dataframe.head())

# Export the DataFrame to a CSV file in the current directory
enforcement_dataframe.to_csv('enforcement_actions.csv', index=False)
```

  
### 2. Crawling (PARTNER 1)

```{python}
# List to hold data with additional 'Agency' field
collected_data_with_agency = []

# Iterate over each enforcement action entry found in parsed HTML
for element in oig_parsed_html.find_all('li', class_='usa-card'):
    title_element = element.find('a')
    title = title_element.text.strip() if title_element else 'No title found'
    link = 'https://oig.hhs.gov' + title_element['href'] if title_element else 'No link found'
    
    # Fetch and parse the linked page for agency information
    if title_element:
        agency_response = requests.get(link)
        agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
        agency_info = agency_soup.find(lambda tag: tag.name == "span" and 'Agency:' in tag.text)
        agency_name = agency_info.next_sibling.text.strip() if agency_info and agency_info.next_sibling else 'No agency found'
    else:
        agency_name = 'No agency found'

    # Use previously parsed date and category information
    date_element = element.find('span', class_='text-base-dark')
    date = date_element.text.strip() if date_element else 'No date found'

    category_element = element.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_element.text.strip() if category_element else 'No category found'

    # Append the additional details to the list
    collected_data_with_agency.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link,
        'Agency': agency_name
    })

# Create a DataFrame and save to CSV
detailed_dataframe = pd.DataFrame(collected_data_with_agency)
print(detailed_dataframe.head())
detailed_dataframe.to_csv('enforcement_actions_detailed.csv', index=False)
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

Define the function and its 2 inputs 

Check for valid year

Use an if statement: if start_year earlier than 2013, print a message telling user its invalid. 
No else statement needed.

Setup base_url and an empty list as in previous code.

Create datetime object from inputs and grab today's date as a datetime object.

Use a While loop. 
The loop should go through the site's pages until it finds a date outside of the range specified. That is, it should start with the current date and move backwards until it finds a date older than the function inputs. But we can't do that here. It makes more sense to do it in the For loop below.
The While loop also needs a counter to move forward through the website's pages.

Use requests.get() to fetch the data from the page.

Then, using the code above as a guide...

Use BeautifulSoup to parse the HTML content of the page. This code to be based on the code above.

Begin a For loop. For each action find:
  Title and Link
  Date
  Category
  Agency Information:
This is where we create the conditions to exit the While loop. If the date is older than the inputs of the function, the function should just return what its got and stop looping.

Create a dictionary of the actions.
Append this dictionary to empty list.

Use time.sleep(1) to wait for 1 second.

Increment the page counter by 1 to go to next page.

After the loop, convert the created list (of dictionaries) to a DataFrame.
Save the DataFrame as a csv.

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
def scrape_function(start_month, start_year):
    # Check that the year is within bounds
    if start_year < 2013:
        print("Please restrict the year to 2013 or later.")
        return
    
    # Define the base URL
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    collected_data = []
    
    # Convert start date to datetime and determine today's date
    start_date = datetime(start_year, start_month, 1)
    today_date = datetime.today()
    
    # Paginate through results until we reach the start_date
    page = 1
    while True:
        # Fetch data for the current page
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        if response.status_code != 200:
            print("Failed to retrieve data.")
            break
        
        # Parse the HTML content
        soup = BeautifulSoup(response.text, 'html.parser')
        actions = soup.find_all('li', class_='usa-card')
        
        # Process each enforcement action item
        for action in actions:
            title_element = action.find('a')
            title = title_element.text.strip() if title_element else 'No title found'
            link = 'https://oig.hhs.gov' + title_element['href'] if title_element else 'No link found'

            # Extract and parse the date
            date_element = action.find('span', class_='text-base-dark')
            date_str = date_element.text.strip() if date_element else 'No date found'
            date_obj = datetime.strptime(date_str, '%B %d, %Y') if date_element else None
            
            # Break out of loop if the date is before start_date
            if date_obj and date_obj < start_date:
                # Convert to DataFrame and return if we've reached the start date
                df = pd.DataFrame(collected_data)
                filename = f"enforcement_actions_{start_year}_{start_month:02d}.csv"
                df.to_csv(filename, index=False)
                print(f"Data saved to {filename}")
                return df
            
            # Extract category
            category_element = action.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
            category = category_element.text.strip() if category_element else 'No category found'
            
            # Visit link to extract agency info
            agency_name = 'No agency found'
            if link != 'No link found':
                agency_response = requests.get(link)
                agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
                agency_info = agency_soup.find(lambda tag: tag.name == "span" and 'Agency:' in tag.text)
                if agency_info and agency_info.next_sibling and hasattr(agency_info.next_sibling, 'text'):
                    agency_name = agency_info.next_sibling.text.strip()

            # Append data
            collected_data.append({
                'Title': title,
                'Date': date_str,
                'Category': category,
                'Link': link,
                'Agency': agency_name
            })
        
        # Wait before going to the next page to avoid server blocks
        time.sleep(1)
        
        # Move to the next page
        page += 1

    # Ensure collected_data is returned as a DataFrame at the end of scraping
    df = pd.DataFrame(collected_data)
    filename = f"enforcement_actions_{start_year}_{start_month:02d}.csv"
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")
    return df

# Example usage of the function
test_df = scrape_function(1, 2023)

# Print total enforcement actions in the DataFrame
print(f"\nTotal enforcement actions: {len(test_df)}")

# Find and print the earliest enforcement action
if not df.empty:
    earliest_action = test_df.sort_values(by='Date').iloc[0]
    print("Earliest Enforcement Action:")
    print(earliest_action)
```

* c. Test Partner's Code (PARTNER 1)

```{python}
# Example usage of the function
partner_test_df = scrape_function(1, 2021)

# Print total enforcement actions in the DataFrame
print(f"\nTotal enforcement actions: {len(partner_test_df)}")

# Find and print the earliest enforcement action
if not df.empty:
    earliest_action_partner = partner_test_df.sort_values(by='Date').iloc[0]
    print("Earliest Enforcement Action:")
    print(earliest_action_partner)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```
