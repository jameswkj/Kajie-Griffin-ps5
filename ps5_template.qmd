---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# Required libraries for web scraping and data handling
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Define the target URL for scraping data
url = 'https://oig.hhs.gov/fraud/enforcement/'

# Initiate a GET request to fetch data from the URL
web_data = requests.get(url)

# Utilize BeautifulSoup to parse the fetched HTML content
parsed_html = BeautifulSoup(web_data.text, 'html.parser')

# Prepare an empty list to hold the data of enforcement actions
collected_data = []

# Iterate over each element in the section containing the data
for element in parsed_html.find_all('li', class_='usa-card'):
    # Locate and clean the title and link, ensuring all entries are captured even if some details are missing
    title_element = element.find('a')
    title = title_element.text.strip() if title_element else 'No title found'
    link = title_element['href'] if title_element else 'No link found'

    # Search for and format the date, managing missing entries effectively
    date_element = element.find('span', class_='text-base-dark')
    date = date_element.text.strip() if date_element else 'No date found'

    # Identify and strip the category, accounting for possible absences
    category_element = element.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_element.text.strip() if category_element else 'No category found'

    # Append the collected details to the list
    collected_data.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link
    })

# Transform the collected list of dictionaries into a DataFrame for better data management
dataframe = pd.DataFrame(collected_data)

# Print the top rows of the DataFrame to verify correct data collection
print(dataframe.head())

# Export the DataFrame to a CSV file in the current directory
dataframe.to_csv('enforcement_actions.csv', index=False)
```

  
### 2. Crawling (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Define the target URL for scraping data
url = 'https://oig.hhs.gov/fraud/enforcement/'

# Initiate a GET request to fetch data from the URL
web_data = requests.get(url)

# Utilize BeautifulSoup to parse the fetched HTML content
parsed_html = BeautifulSoup(web_data.text, 'html.parser')

# Prepare an empty list to hold the data of enforcement actions
collected_data = []

# Iterate over each element in the section containing the data
for element in parsed_html.find_all('li', class_='usa-card'):
    title_element = element.find('a')
    title = title_element.text.strip() if title_element else 'No title found'
    link = 'https://oig.hhs.gov' + title_element['href'] if title_element else 'No link found'

    # Follow the link to scrape the agency name
    agency_response = requests.get(link)
    agency_soup = BeautifulSoup(agency_response.text, 'html.parser')
    # Look for the span containing 'Agency:' and safely extract the following sibling's text
    agency_info = agency_soup.find(lambda tag: tag.name == "span" and 'Agency:' in tag.text)
    if agency_info and agency_info.next_sibling and hasattr(agency_info.next_sibling, 'text'):
        agency_name = agency_info.next_sibling.text.strip()
    else:
        agency_name = 'No agency found'

    date_element = element.find('span', class_='text-base-dark')
    date = date_element.text.strip() if date_element else 'No date found'

    category_element = element.find('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_element.text.strip() if category_element else 'No category found'

    collected_data.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link,
        'Agency': agency_name
    })

# Transform the collected list of dictionaries into a DataFrame for better data management
dataframe = pd.DataFrame(collected_data)

# Print the top rows of the DataFrame to verify correct data collection
print(dataframe.head())

# Export the DataFrame to a CSV file in the current directory
dataframe.to_csv('enforcement_actions_detailed.csv', index=False)
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```
